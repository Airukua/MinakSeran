{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427e9dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['aku fas motor a tura maniwa ra']\n",
      "Tokens: ['aku fas motora tura maniwara']\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "from seram_tokenizer.paragog_normaliser import GeserParagogNormalizer\n",
    "\n",
    "# Example usage of the GeserParagogNormalizer\n",
    "text = ['aku fas motor a tura maniwa ra']\n",
    "\n",
    "# Initialize the normalizer with the text\n",
    "normalizer = GeserParagogNormalizer(text)\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalizer.normalize()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78d3d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: aku nugu ngasana habiba, aku atamari wanu karay.\n",
      "Tokens: ['aku', 'nugu', 'ngasana', 'habiba', ',', 'aku', 'atamari', 'wanu', 'karay', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "from seram_tokenizer import SeramTokenizer\n",
    "\n",
    "# Text in Seram language\n",
    "text = \"aku nugu ngasana habiba, aku atamari wanu karay.\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = SeramTokenizer(text)\n",
    "\n",
    "# Perform tokenization\n",
    "tokens = tokenizer.tokenize()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13213aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# dataset = 'dataset_seram_geser.csv'\n",
    "# df = pd.read_csv(dataset)\n",
    "# df.columns = ['Indonesian', 'Geser']\n",
    "# text = df['Geser'].tolist()\n",
    "# filtered_text = [s for s in text if len(s.split()) >= 2]\n",
    "# normalizer = GeserParagogNormalizer(filtered_text)\n",
    "# text_normalizer = normalizer.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9391139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natagi']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"i natagi bua sikolara\"\n",
    "pattern = re.compile('na\\w+')\n",
    "matches = pattern.findall(text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36ab630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tura', 'maniwara']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Membaca daftar kata dari file\n",
    "with open('seram_tokenizer/data/geser_word.txt', 'r', encoding='utf-8') as file:\n",
    "    word_list = file.read().splitlines()\n",
    "\n",
    "# Pola regex untuk kata yang diakhiri dengan 'ra'\n",
    "ra_suffix_pattern = re.compile(r'\\w+ra$')\n",
    "\n",
    "# Ambil hanya entri yang terdiri dari satu kata\n",
    "single_word_entries = [word for word in word_list if len(word.split()) == 1]\n",
    "\n",
    "# Cari kata yang berakhiran 'ra' dalam daftar\n",
    "words_ending_with_ra = [word for word in single_word_entries if ra_suffix_pattern.search(word)]\n",
    "\n",
    "# Contoh teks untuk diuji\n",
    "text = \"aku fas motor a tura maniwara\"\n",
    "text_split = text.split()\n",
    "\n",
    "# Cari kata dalam teks yang cocok dengan pola akhiran 'ra'\n",
    "ra_words = [word for word in text_split if ra_suffix_pattern.search(word)]\n",
    "\n",
    "# Ambil kata yang tidak ada dalam daftar words_ending_with_ra\n",
    "final_words = [word for word in ra_words if word not in words_ending_with_ra]\n",
    "final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f7fcc2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tura', 'maniwara']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'aku fas motor a tura maniwara'\n",
    "lower = text.lower()\n",
    "text_split = lower.split()\n",
    "ra_words = [word for word in text_split if ra_suffix_pattern.search(word)]\n",
    "ra_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4603c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from typing import List, Set\n",
    "import pkg_resources\n",
    "import re\n",
    "\n",
    "# INSPIRED BY The implementation of https://github.com/AbdullahAlabbas/Wordle/blob/main/play_wordle.py\n",
    "\n",
    "def load_word_set(file_path: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load words from a file into a set for O(1) lookup performance.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file containing words\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: Set of words from the file\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found at: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return {line.strip() for line in f if line.strip()}\n",
    "\n",
    "\n",
    "# Define file paths using pkg_resources to ensure compatibility with package data\n",
    "DICTIONARY_FILE_PATH = pkg_resources.resource_filename(__name__, 'seram_tokenizer/data/geser_word.txt')\n",
    "VOCAL_FILE_PATH = pkg_resources.resource_filename(__name__, 'seram_tokenizer/data/vocal.txt')\n",
    "CONSONANT_FILE_PATH = pkg_resources.resource_filename(__name__, 'seram_tokenizer/data/consonant.txt')\n",
    "\n",
    "# INITIALIZE SETS\n",
    "DICTIONARY_WORDS: Set[str] = set()\n",
    "VOCAL_LETTERS: Set[str] = set()\n",
    "CONSONANT_LETTERS: Set[str] = set()\n",
    "\n",
    "# Load Dictionary, Vocal, and Consonant letters from files\n",
    "try:\n",
    "    DICTIONARY_WORDS = load_word_set(DICTIONARY_FILE_PATH)\n",
    "    VOCAL_LETTERS = load_word_set(VOCAL_FILE_PATH)\n",
    "    CONSONANT_LETTERS = load_word_set(CONSONANT_FILE_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure all necessary files exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Compiled regex patterns for better performance\n",
    "RA_PATTERN = re.compile(r'\\w+ra$')\n",
    "A_PATTERN = re.compile(r'\\w+a$')\n",
    "\n",
    "# Pre-filter dictionary words for efficiency - computed once at module load\n",
    "SINGLE_WORD_DICTIONARY_ENTRIES: Set[str] = {\n",
    "    word for word in DICTIONARY_WORDS \n",
    "    if ' ' not in word  \n",
    "}\n",
    "\n",
    "LEMMA_WITH_RA: Set[str] = {\n",
    "    word for word in SINGLE_WORD_DICTIONARY_ENTRIES \n",
    "    if word.endswith('ra')\n",
    "}\n",
    "\n",
    "LEMMA_WITH_A: Set[str] = {\n",
    "    word for word in SINGLE_WORD_DICTIONARY_ENTRIES \n",
    "    if word.endswith('a')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f92af26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arumbaura',\n",
       " 'asara',\n",
       " 'balara',\n",
       " 'bora',\n",
       " 'butira',\n",
       " 'cara',\n",
       " 'dadafira',\n",
       " 'datara',\n",
       " 'gambara',\n",
       " 'gambira',\n",
       " 'gembira',\n",
       " 'inu-inura',\n",
       " 'ira',\n",
       " 'jahera',\n",
       " 'jambura',\n",
       " 'juara',\n",
       " 'kayara',\n",
       " 'lebara',\n",
       " 'madiara',\n",
       " 'malura',\n",
       " 'manira',\n",
       " 'matara',\n",
       " 'mera',\n",
       " 'mumura',\n",
       " 'naira',\n",
       " 'nanara',\n",
       " 'natara',\n",
       " 'nyira',\n",
       " 'odi-odira',\n",
       " 'pera',\n",
       " 'rantira',\n",
       " 'rara',\n",
       " 'salompara',\n",
       " 'sapatura',\n",
       " 'sarasara',\n",
       " 'sawara',\n",
       " 'sibura',\n",
       " 'sikarura',\n",
       " 'sinelara',\n",
       " 'sira',\n",
       " 'sukara',\n",
       " 'tabara',\n",
       " 'tetewara',\n",
       " 'tinira',\n",
       " 'tinumura',\n",
       " 'togira',\n",
       " 'tonsoara',\n",
       " 'topira',\n",
       " 'udara',\n",
       " 'yayaira'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEMMA_WITH_RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e56fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
